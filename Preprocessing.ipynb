{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkmS4hPbcjEi",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing \n",
        "\n",
        "In this notebook all the preprocessing phase is computed. The \"***cleaned***\" version of the datasets are created and saved to new csv files. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TDR_raAhC7K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "000f9a17-c62a-4a92-fa05-1da378ea378d"
      },
      "source": [
        "# google drive settings \n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFTVuUgXg-Bn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# google drive settings \n",
        "%%capture\n",
        "%cd /content/gdrive/My\\ Drive/NLP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU-vD8QbgzJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture \n",
        "!python3 -m spacy download it_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk2WAewqi8CC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "import spacy\n",
        "import it_core_news_sm\n",
        "nlp = it_core_news_sm.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-cVg7qmgpTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_tweet(text):   \n",
        "  #remove urls \n",
        "  urls_pattern = 'http\\S+'\n",
        "  clean_version = re.sub(urls_pattern, ' ', text)\n",
        "\n",
        "  #replace hashtag symbol with whitespace\n",
        "  hashtag_pattern = '#(\\w+)'\n",
        "  clean_version = re.sub(hashtag_pattern, ' ', clean_version)\n",
        "\n",
        "  #replace - with whitespace\n",
        "  clean_version = clean_version.replace('-', ' ')\n",
        "\n",
        "  clean_version = clean_version.replace('_', ' ')\n",
        "\n",
        "  #replace \\ with whitespace\n",
        "  clean_version = clean_version.replace('\\'' , ' ')\n",
        "\n",
        "  #remove \\n \n",
        "  clean_version = clean_version.replace('\\n', ' ')\n",
        "\n",
        "  #remove emoji \n",
        "  emoji_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "  clean_version = emoji_pattern.sub(' ',clean_version)\n",
        "\n",
        "  #remove numbers \n",
        "  numbers_pattern = '\\d+'\n",
        "  clean_version = re.sub(numbers_pattern, ' ', clean_version)\n",
        "\n",
        "  #remove punctuation (remove also @)\n",
        "  punt_pattern = '[^\\w\\s]'\n",
        "  clean_version = re.sub(punt_pattern,' ',clean_version)\n",
        "\n",
        "  # remove multiple whitespace \n",
        "  clean_version = re.sub(' +', ' ', clean_version )\n",
        "\n",
        "  return clean_version.lower()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAZnRmOYd5ub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# retrieve italian stopwords from file \n",
        "stop_word_file = open(\"tweet_data/italian_stop_words.txt\", \"r\")\n",
        "stopwords_italian = stop_word_file.read().split('\\n')\n",
        "set_word = set(stopwords_italian) #transform it to set to allow faster evaluation\n",
        "\n",
        "def lemmatize_text(text, stopword_list = set_word): \n",
        "  final_words = [] \n",
        "  lemmas = [token.lemma_ for token in nlp(text) \n",
        "              if token.pos_ in {'ADJ', 'ADV', 'NOUN', 'NUMERAL', 'NUM', 'PROPN','VERB'}]\n",
        "  for word in lemmas:  \n",
        "    if word not in stopword_list: \n",
        "      final_words.append(word)\n",
        "  return (\" \".join(final_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik5IpVs4gvHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find the hashtag given a tweet \n",
        "def find_hashtag(tweet):\n",
        "  hashtag_pattern = '#(\\w+)'\n",
        "  return re.findall(hashtag_pattern, tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYEXBlsxifLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first week dataset --  Sunday 23 Feb - Sunday 1 March\n",
        "feb23 = pd.read_csv(\"tweet_data/Feb23.csv\")\n",
        "feb24 = pd.read_csv(\"tweet_data/Feb24.csv\")\n",
        "feb25 = pd.read_csv(\"tweet_data/Feb25.csv\")\n",
        "feb26 = pd.read_csv(\"tweet_data/Feb26.csv\")\n",
        "feb27 = pd.read_csv(\"tweet_data/Feb27.csv\")\n",
        "feb28 = pd.read_csv(\"tweet_data/Feb28.csv\")\n",
        "feb29 = pd.read_csv(\"tweet_data/Feb29.csv\")\n",
        "march1 = pd.read_csv(\"tweet_data/March01.csv\")\n",
        "first_week = pd.concat([feb23, feb24, feb25, feb26, feb27, feb28, feb29, march1], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUrlsI8trina",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7fd999c4-17b2-4820-9f38-3afe4293e0dd"
      },
      "source": [
        "%%time\n",
        "first_week['clean_tweet'] =  first_week['text'].apply(clean_tweet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.31 s, sys: 34.5 ms, total: 4.34 s\n",
            "Wall time: 4.35 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f24ZuCC6zgFX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d24dacf0-b701-4e65-c1d9-694ba04c28c9"
      },
      "source": [
        "%%time\n",
        "first_week['clean_text'] = first_week['clean_tweet'].apply(lemmatize_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 29min 2s, sys: 3.28 s, total: 29min 5s\n",
            "Wall time: 29min 6s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izzgALehzhLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_week['hashtag'] = first_week['text'].apply(find_hashtag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bAXOSjOWpow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columms = ['text', 'clean_text', 'clean_tweet','hashtag', 'date', 'year', 'month', 'day', 'hour']\n",
        "first_week[columms].to_csv('tweet_data/first_week_new.csv', index=False, quoting=csv.QUOTE_ALL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuHeYpb7lJNm",
        "colab_type": "text"
      },
      "source": [
        "Second week -- Sunday 15 March - Sunday 22 March\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwQunBXGsIoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mar15 = pd.read_csv(\"tweet_data/March15.csv\")\n",
        "mar16 = pd.read_csv(\"tweet_data/March16.csv\")\n",
        "mar17 = pd.read_csv(\"tweet_data/March17.csv\")\n",
        "mar18 = pd.read_csv(\"tweet_data/March18.csv\")\n",
        "mar19 = pd.read_csv(\"tweet_data/March19.csv\")\n",
        "mar20 = pd.read_csv(\"tweet_data/March20.csv\")\n",
        "mar21 = pd.read_csv(\"tweet_data/March21.csv\")\n",
        "mar22 = pd.read_csv(\"tweet_data/March22.csv\")\n",
        "second_week = pd.concat([mar15, mar16, mar17, mar18, mar19, mar20, mar21, mar22], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUoKl0DbsLfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "second_week['clean_tweet'] =  second_week['text'].apply(clean_tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uNU5qTMsQg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "second_week['clean_text'] = second_week['clean_tweet'].apply(lemmatize_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su-9sRowsM79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "second_week['hashtag'] = second_week['text'].apply(find_hashtag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx7ULLXxsOvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columms = ['text', 'clean_text', 'clean_tweet','hashtag', 'date', 'year', 'month', 'day', 'hour']\n",
        "second_week[columms].to_csv('tweet_data/second_week_new.csv', index=False,  quoting=csv.QUOTE_ALL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soc9ywqKxgPD",
        "colab_type": "text"
      },
      "source": [
        "Third week -- Sunday 19 April - Sunday 26 April\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2VjaHCmkcui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# third week dataset -- Sunday 19 April - Sunday 26 April \n",
        "apr19 = pd.read_csv(\"tweet_data/April19.csv\")\n",
        "apr20 = pd.read_csv(\"tweet_data/April20.csv\")\n",
        "apr21 = pd.read_csv(\"tweet_data/April21.csv\")\n",
        "apr22 = pd.read_csv(\"tweet_data/April22.csv\")\n",
        "apr23 = pd.read_csv(\"tweet_data/April23.csv\")\n",
        "apr24 = pd.read_csv(\"tweet_data/April24.csv\")\n",
        "apr25 = pd.read_csv(\"tweet_data/April25.csv\")\n",
        "apr26 = pd.read_csv(\"tweet_data/April26.csv\")\n",
        "third_week = pd.concat([apr19, apr20, apr21, apr22, apr23, apr24, apr25, apr26], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iUqvL_srpoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "third_week['clean_tweet'] =  third_week['text'].apply(clean_tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgt-O0RTrk7Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9f5c9a66-e952-4461-8847-cd6a83f7312f"
      },
      "source": [
        "%%time\n",
        "third_week['clean_text'] = third_week['clean_tweet'].apply(lemmatize_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 27min 54s, sys: 2.91 s, total: 27min 57s\n",
            "Wall time: 27min 58s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTuXrnrnrkt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "third_week['hashtag'] = third_week['text'].apply(find_hashtag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUngm8DorkAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columms = ['text', 'clean_text', 'clean_tweet','hashtag', 'date', 'year', 'month', 'day', 'hour']\n",
        "third_week[columms].to_csv('tweet_data/third_week_new.csv', index=False, quoting=csv.QUOTE_ALL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glYKhU6alTUy",
        "colab_type": "text"
      },
      "source": [
        "Fourth week -- Sunday 17 May - Sunday 24 May"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8YLzJFaifIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fourth week dataset-- Sunday 17 May - Sunday 24 May \n",
        "may17 = pd.read_csv(\"tweet_data/May17.csv\")\n",
        "may18 = pd.read_csv(\"tweet_data/May18.csv\")\n",
        "may19 = pd.read_csv(\"tweet_data/May19.csv\")\n",
        "may20 = pd.read_csv(\"tweet_data/May20.csv\")\n",
        "may21 = pd.read_csv(\"tweet_data/May21.csv\")\n",
        "may22 = pd.read_csv(\"tweet_data/May22.csv\")\n",
        "may23 = pd.read_csv(\"tweet_data/May23.csv\")\n",
        "may24 = pd.read_csv(\"tweet_data/May24.csv\")\n",
        "fourth_week = pd.concat([may17, may18, may19, may20, may21, may22, may23, may24], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JtOVsWwrqPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c0354eb1-00f5-407d-a193-c0dc9ffea57c"
      },
      "source": [
        "%%time\n",
        "fourth_week['clean_tweet'] =  fourth_week['text'].apply(clean_tweet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2.03 s, sys: 18 ms, total: 2.05 s\n",
            "Wall time: 2.06 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_87UFD_ie44",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1bf63b49-a857-48cc-ea34-8d916ec24544"
      },
      "source": [
        "%%time\n",
        "fourth_week['clean_text'] = fourth_week['clean_tweet'].apply(lemmatize_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 14min 8s, sys: 1.73 s, total: 14min 10s\n",
            "Wall time: 14min 11s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LO2I_Lliez6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fourth_week['hashtag'] = fourth_week['text'].apply(find_hashtag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWbDao0jiery",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columms = ['text', 'clean_text', 'clean_tweet','hashtag', 'date', 'year', 'month', 'day', 'hour']\n",
        "fourth_week[columms].to_csv('tweet_data/fourth_week_new.csv', index=False,  quoting=csv.QUOTE_ALL)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}